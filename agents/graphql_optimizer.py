#!/usr/bin/env python3
"""
GraphQL Performance Optimizer - Complete Implementation
Phase 1: Core Optimizations
Phase 2: AI-Powered Features  
Phase 3: Production Hardening
"""

import json
import time
import hashlib
import asyncio
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Callable
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from collections import defaultdict
from enum import Enum
import threading
from contextlib import contextmanager
import weakref

# ============================================
# DATACLASSES
# ============================================

class CacheType(Enum):
    IN_MEMORY = "memory"
    REDIS = "redis"
    DISK = "disk"

class QueryComplexity(Enum):
    LOW = "low"      # < 10 points
    MEDIUM = "medium" # 10-50 points
    HIGH = "high"     # 50-100 points
    CRITICAL = "critical" # > 100 points

@dataclass
class QueryMetrics:
    """Metrics for a single query execution."""
    query_hash: str
    operation_name: str
    execution_time_ms: float
    resolver_count: int
    db_queries: int
    cache_hits: int
    cache_misses: int
    memory_bytes: int
    error_count: int
    timestamp: datetime
    variables: Dict[str, Any]
    complexity: QueryComplexity
    cache_key: str
    
    @property
    def cache_hit_rate(self) -> float:
        total = self.cache_hits + self.cache_misses
        return (self.cache_hits / total * 100) if total > 0 else 0
    
    @property
    def cost_score(self) -> int:
        """Calculate cost score for the query."""
        base = self.db_queries * 2
        base += self.resolver_count
        base += int(self.execution_time_ms / 10)
        base += self.error_count * 5
        return base

@dataclass
class DataLoaderConfig:
    """Configuration for DataLoader."""
    batch_size: int = 100
    max_batch_size: int = 500
    cache_ttl_seconds: int = 300
    enable_request_cache: bool = True
    enable_persistent_cache: bool = False
    cache_key_prefix: str = "graphql:loader"

@dataclass 
class CacheConfig:
    """Cache configuration."""
    default_ttl: int = 300
    max_size_mb: int = 100
    eviction_policy: str = "lru"  # lru, fifo, lfu
    enable_compression: bool = True
    compression_algorithm: str = "gzip"
    redis_url: Optional[str] = None
    local_cache_size: int = 10000

@dataclass
class QueryConfig:
    """Query execution configuration."""
    max_depth: int = 7
    max_complexity: int = 100
    max_execution_time_ms: int = 5000
    max_db_queries: int = 50
    max_resolvers: int = 100
    enable_query_caching: bool = True
    enable_response_caching: bool = True
    cache_ttl: int = 60

@dataclass
class MonitoringConfig:
    """Monitoring configuration."""
    enable_metrics: bool = True
    metrics_interval_seconds: int = 60
    slow_query_threshold_ms: int = 1000
    error_rate_threshold: float = 0.05
    enable_alerting: bool = True
    alert_webhook: Optional[str] = None
    enable_dashboard: bool = True
    dashboard_port: int = 9090

# ============================================
# DATA LOADER IMPLEMENTATION
# ============================================

class BatchResult:
    """Result of a batch operation."""
    def __init__(self, results: List[Any], errors: List[Exception] = None):
        self.results = results
        self.errors = errors or []
        self.timestamp = datetime.now()

class DataLoader:
    """
    Batch loader for N+1 query prevention.
    Batches multiple requests into single database query.
    """
    
    def __init__(self, name: str, batch_fn: Callable[[List[str]], List[Any]], config: DataLoaderConfig = None):
        self.name = name
        self.batch_fn = batch_fn
        self.config = config or DataLoaderConfig()
        
        # Pending loads (batched within event loop tick)
        self._pending: Dict[str, Any] = {}
        self._pending_count = 0
        
        # Caches
        self._request_cache: Dict[str, Any] = {}  # Cleared per request
        self._cache: LRUCache = LRUCache(max_size=10000)
        
        # Statistics
        self._stats = {
            "batches_executed": 0,
            "total_loaded": 0,
            "cache_hits": 0,
            "cache_misses": 0
        }
        
        # Thread safety
        self._lock = threading.Lock()
    
    async def load(self, key: str) -> Any:
        """Load a single item."""
        # Check request cache first
        if self.config.enable_request_cache and key in self._request_cache:
            self._stats["cache_hits"] += 1
            return self._request_cache[key]
        
        # Check persistent cache
        cache_key = f"{self.name}:{key}"
        cached = self._cache.get(cache_key)
        if cached:
            self._stats["cache_hits"] += 1
            self._request_cache[key] = cached
            return cached
        
        # Queue for batch
        self._stats["cache_misses"] += 1
        if key not in self._pending:
            self._pending[key] = asyncio.Future()
            self._pending_count += 1
        
        # Execute batch if threshold reached
        if self._pending_count >= self.config.batch_size:
            await self._execute_batch()
        
        # Wait for result
        result = await self._pending[key]
        self._request_cache[key] = result
        return result
    
    async def load_many(self, keys: List[str]) -> List[Any]:
        """Load multiple items."""
        results = []
        for key in keys:
            results.append(await self.load(key))
        return results
    
    async def _execute_batch(self):
        """Execute batched requests."""
        if not self._pending:
            return
        
        keys = list(self._pending.keys())
        futures = [self._pending[k] for k in keys]
        
        try:
            # Execute batch function
            batch_results = await self.batch_fn(keys)
            
            # Set results
            for key, result in zip(keys, batch_results):
                cache_key = f"{self.name}:{key}"
                self._cache.set(cache_key, result, ttl=self.config.cache_ttl_seconds)
                self._request_cache[key] = result
                self._pending[key].set_result(result)
            
            self._stats["batches_executed"] += 1
            self._stats["total_loaded"] += len(keys)
            
        except Exception as e:
            # Set error on all pending futures
            for key, future in zip(keys, futures):
                if not future.done():
                    future.set_exception(e)
        
        # Clear pending
        self._pending.clear()
        self._pending_count = 0
    
    def clear_request_cache(self):
        """Clear request-scoped cache."""
        self._request_cache.clear()
    
    def get_stats(self) -> Dict:
        """Get loader statistics."""
        total = self._stats["cache_hits"] + self._stats["cache_misses"]
        hit_rate = (self._stats["cache_hits"] / total * 100) if total > 0 else 0
        
        return {
            "name": self.name,
            "batches_executed": self._stats["batches_executed"],
            "total_loaded": self._stats["total_loaded"],
            "cache_hits": self._stats["cache_hits"],
            "cache_misses": self._stats["cache_misses"],
            "cache_hit_rate": f"{hit_rate:.1f}%",
            "cache_size": self._cache.size()
        }

# ============================================
# CACHE IMPLEMENTATIONS
# ============================================

class LRUCache:
    """Thread-safe LRU Cache."""
    
    def __init__(self, max_size: int = 10000):
        self._max_size = max_size
        self._cache: Dict[str, Any] = {}
        self._order: List[str] = []
        self._lock = threading.Lock()
    
    def get(self, key: str) -> Optional[Any]:
        """Get item from cache."""
        with self._lock:
            if key in self._cache:
                # Move to end (most recently used)
                self._order.remove(key)
                self._order.append(key)
                return self._cache[key]
            return None
    
    def set(self, key: str, value: Any, ttl: int = None):
        """Set item in cache."""
        with self._lock:
            if key in self._cache:
                self._order.remove(key)
            elif len(self._order) >= self._max_size:
                # Evict oldest
                evict_key = self._order.pop(0)
                del self._cache[evict_key]
            
            self._cache[key] = {
                "value": value,
                "expires": datetime.now() + timedelta(seconds=ttl) if ttl else None,
                "created": datetime.now()
            }
            self._order.append(key)
    
    def delete(self, key: str) -> bool:
        """Delete item from cache."""
        with self._lock:
            if key in self._cache:
                self._order.remove(key)
                del self._cache[key]
                return True
            return False
    
    def clear(self):
        """Clear all items."""
        with self._lock:
            self._cache.clear()
            self._order.clear()
    
    def size(self) -> int:
        """Get cache size."""
        with self._lock:
            return len(self._cache)
    
    def get_stats(self) -> Dict:
        """Get cache statistics."""
        with self._lock:
            return {
                "size": len(self._cache),
                "max_size": self._max_size,
                "utilization": f"{len(self._cache) / self._max_size * 100:.1f}%"
            }

class MultiLevelCache:
    """
    Multi-level cache with L1 (memory) and L2 (Redis/disk).
    """
    
    def __init__(self, config: CacheConfig):
        self.config = config
        self._l1 = LRUCache(max_size=config.local_cache_size)
        
        # L2 cache (Redis if configured)
        self._redis = None
        self._redis_available = False
        self._init_redis()
        
        # Statistics
        self._stats = {
            "l1_hits": 0,
            "l2_hits": 0,
            "misses": 0,
            "sets": 0
        }
    
    def _init_redis(self):
        """Initialize Redis connection."""
        if self.config.redis_url:
            try:
                import redis
                self._redis = redis.from_url(self.config.redis_url)
                self._redis.ping()
                self._redis_available = True
            except Exception as e:
                logging.warning(f"Redis unavailable: {e}")
                self._redis_available = False
    
    def get(self, key: str) -> Optional[Any]:
        """Get from multi-level cache."""
        # L1 (memory)
        l1_result = self._l1.get(key)
        if l1_result:
            self._stats["l1_hits"] += 1
            return l1_result["value"]
        
        # L2 (Redis)
        if self._redis_available:
            try:
                l2_result = self._redis.get(key)
                if l2_result:
                    self._stats["l2_hits"] += 1
                    # Promote to L1
                    data = json.loads(l2_result)
                    self._l1.set(key, data["value"])
                    return data["value"]
            except Exception:
                pass
        
        self._stats["misses"] += 1
        return None
    
    def set(self, key: str, value: Any, ttl: int = None):
        """Set in multi-level cache."""
        ttl = ttl or self.config.default_ttl
        data = {
            "value": value,
            "expires": (datetime.now() + timedelta(seconds=ttl)).isoformat()
        }
        
        # L1
        self._l1.set(key, value, ttl=ttl)
        
        # L2
        if self._redis_available:
            try:
                self._redis.setex(key, ttl, json.dumps(data))
            except Exception:
                pass
        
        self._stats["sets"] += 1
    
    def delete(self, key: str) -> bool:
        """Delete from all cache levels."""
        l1_deleted = self._l1.delete(key)
        
        if self._redis_available:
            try:
                self._redis.delete(key)
            except Exception:
                pass
        
        return l1_deleted
    
    def get_stats(self) -> Dict:
        """Get cache statistics."""
        total = self._stats["l1_hits"] + self._stats["l2_hits"] + self._stats["misses"]
        return {
            "l1_hits": self._stats["l1_hits"],
            "l2_hits": self._stats["l2_hits"],
            "misses": self._stats["misses"],
            "l1_size": self._l1.size(),
            "total_requests": total,
            "overall_hit_rate": f"{(total - self._stats['misses']) / total * 100:.1f}%" if total > 0 else "N/A"
        }

# ============================================
# QUERY ANALYZER
# ============================================

class QueryComplexityAnalyzer:
    """Analyzes GraphQL query complexity."""
    
    DEFAULT_COSTS = {
        "FIELD": 1,
        "OBJECT": 2,
        "LIST": 10,
        "SCALAR": 1,
        "CONNECTION": 2,
    }
    
    DEPTH_LIMITS = {
        QueryComplexity.LOW: 5,
        QueryComplexity.MEDIUM: 7,
        QueryComplexity.HIGH: 10,
        QueryComplexity.CRITICAL: 12
    }
    
    def __init__(self, max_depth: int = 7, max_complexity: int = 100):
        self.max_depth = max_depth
        self.max_complexity = max_complexity
    
    def analyze(self, query: str, variables: Dict = None) -> Dict:
        """Analyze query complexity and depth."""
        return {
            "depth": self._calculate_depth(query),
            "complexity": self._calculate_complexity(query),
            "is_valid": True,
            "warnings": [],
            "recommendations": []
        }
    
    def _calculate_depth(self, query: str) -> int:
        """Calculate maximum nesting depth."""
        depth = 0
        max_depth = 0
        
        for char in query:
            if char == '{':
                depth += 1
                max_depth = max(max_depth, depth)
            elif char == '}':
                depth -= 1
        
        return max_depth
    
    def _calculate_complexity(self, query: str) -> int:
        """Estimate query complexity score."""
        # Simplified complexity calculation
        complexity = query.count('{') * 2
        complexity += query.count('query') * 10
        complexity += query.count('fragment') * 5
        return complexity
    
    def should_allow(self, query: str, variables: Dict = None) -> tuple[bool, str]:
        """Determine if query should be allowed."""
        analysis = self.analyze(query, variables)
        
        if analysis["depth"] > self.max_depth:
            return False, f"Query depth {analysis['depth']} exceeds limit {self.max_depth}"
        
        if analysis["complexity"] > self.max_complexity:
            return False, f"Query complexity {analysis['complexity']} exceeds limit {self.max_complexity}"
        
        return True, "Query allowed"

# ============================================
# PERFORMANCE MONITOR
# ============================================

class PerformanceMonitor:
    """Real-time performance monitoring for GraphQL."""
    
    def __init__(self, config: MonitoringConfig = None):
        self.config = config or MonitoringConfig()
        
        # Metrics storage
        self._queries: List[QueryMetrics] = []
        self._lock = threading.Lock()
        
        # Aggregated metrics
        self._aggregated = {
            "total_queries": 0,
            "avg_execution_time_ms": 0,
            "total_errors": 0,
            "cache_hit_rate": 0,
            "queries_per_second": 0
        }
        
        # Slow query threshold
        self.slow_query_threshold_ms = config.slow_query_threshold_ms if config else 1000
        
        # Alert callbacks
        self._alert_callbacks: List[Callable] = []
        
        # Start monitoring
        self._running = True
        self._metrics_thread = threading.Thread(target=self._aggregate_loop)
        self._metrics_thread.daemon = True
        self._metrics_thread.start()
    
    def record_query(self, metrics: QueryMetrics):
        """Record a query execution."""
        with self._lock:
            self._queries.append(metrics)
            
            # Keep only last 10000 queries
            if len(self._queries) > 10000:
                self._queries = self._queries[-10000:]
            
            # Check for slow queries
            if metrics.execution_time_ms > self.slow_query_threshold_ms:
                self._alert(f"Slow query detected: {metrics.execution_time_ms:.2f}ms for {metrics.operation_name}")
            
            # Check error rate
            if metrics.error_count > 0:
                self._alert(f"Query with errors: {metrics.operation_name}")
    
    def register_alert_callback(self, callback: Callable[[str], None]):
        """Register alert callback."""
        self._alert_callbacks.append(callback)
    
    def _alert(self, message: str):
        """Send alert."""
        for callback in self._alert_callbacks:
            try:
                callback(message)
            except Exception:
                pass
    
    def _aggregate_loop(self):
        """Aggregate metrics in background."""
        while self._running:
            time.sleep(self.config.metrics_interval_seconds)
            self._aggregate()
    
    def _aggregate(self):
        """Aggregate recent metrics."""
        with self._lock:
            if not self._queries:
                return
            
            now = datetime.now()
            recent = [q for q in self._queries 
                     if now - q.timestamp < timedelta(minutes=5)]
            
            if not recent:
                return
            
            # Calculate aggregates
            total_time = sum(q.execution_time_ms for q in recent)
            total_errors = sum(q.error_count for q in recent)
            total_cache_hits = sum(q.cache_hits for q in recent)
            total_cache = sum(q.cache_hits + q.cache_misses for q in recent)
            
            self._aggregated = {
                "total_queries": len(recent),
                "avg_execution_time_ms": total_time / len(recent),
                "total_errors": total_errors,
                "cache_hit_rate": (total_cache_hits / total_cache * 100) if total_cache > 0 else 0,
                "queries_per_second": len(recent) / 300
            }
    
    def get_metrics(self) -> Dict:
        """Get current metrics."""
        with self._lock:
            return {
                "aggregated": self._aggregated,
                "recent_queries": len([q for q in self._queries 
                                     if datetime.now() - q.timestamp < timedelta(seconds=60)])
            }
    
    def get_slow_queries(self, limit: int = 10) -> List[QueryMetrics]:
        """Get slowest queries."""
        with self._lock:
            return sorted(self._queries, 
                        key=lambda q: q.execution_time_ms, 
                        reverse=True)[:limit]
    
    def get_query_distribution(self) -> Dict:
        """Get query complexity distribution."""
        with self._lock:
            distribution = {k.value: 0 for k in QueryComplexity}
            for q in self._queries:
                distribution[q.complexity.value] += 1
            return distribution

# ============================================
# CIRCUIT BREAKER
# ============================================

class CircuitState(Enum):
    CLOSED = "closed"
    OPEN = "open"
    HALF_OPEN = "half_open"

class CircuitBreaker:
    """
    Circuit breaker for protecting against cascade failures.
    """
    
    def __init__(self, name: str, failure_threshold: int = 5, 
                 recovery_timeout_seconds: int = 30):
        self.name = name
        self.failure_threshold = failure_threshold
        self.recovery_timeout = timedelta(seconds=recovery_timeout_seconds)
        
        self._state = CircuitState.CLOSED
        self._failure_count = 0
        self._last_failure: Optional[datetime] = None
        self._lock = threading.Lock()
    
    @property
    def state(self) -> CircuitState:
        """Get current state."""
        if self._state == CircuitState.OPEN:
            if self._last_failure and datetime.now() - self._last_failure > self.recovery_timeout:
                self._state = CircuitState.HALF_OPEN
        return self._state
    
    def call(self, fn: Callable, *args, **kwargs) -> Any:
        """Execute function with circuit breaker protection."""
        if self.state == CircuitState.OPEN:
            raise CircuitOpenError(f"Circuit {self.name} is open")
        
        try:
            result = fn(*args, **kwargs)
            self._on_success()
            return result
        except Exception as e:
            self._on_failure()
            raise
    
    def _on_success(self):
        """Handle successful call."""
        with self._lock:
            if self._state == CircuitState.HALF_OPEN:
                self._state = CircuitState.CLOSED
                self._failure_count = 0
    
    def _on_failure(self):
        """Handle failed call."""
        with self._lock:
            self._failure_count += 1
            self._last_failure = datetime.now()
            
            if self._failure_count >= self.failure_threshold:
                self._state = CircuitState.OPEN
    
    def reset(self):
        """Reset circuit breaker."""
        with self._lock:
            self._state = CircuitState.CLOSED
            self._failure_count = 0
            self._last_failure = None

class CircuitOpenError(Exception):
    """Raised when circuit breaker is open."""
    pass

# ============================================
# MAIN OPTIMIZER CLASS
# ============================================

class GraphQLEPerformanceOptimizer:
    """
    Complete GraphQL Performance Optimizer.
    Combines all components for production-ready optimization.
    """
    
    def __init__(self, 
                 query_config: QueryConfig = None,
                 cache_config: CacheConfig = None,
                 monitoring_config: MonitoringConfig = None):
        
        self.query_config = query_config or QueryConfig()
        self.cache_config = cache_config or CacheConfig()
        self.monitoring_config = monitoring_config or MonitoringConfig()
        
        # Core components
        self.cache = MultiLevelCache(self.cache_config)
        self.monitor = PerformanceMonitor(self.monitoring_config)
        self.complexity_analyzer = QueryComplexityAnalyzer(
            max_depth=self.query_config.max_depth,
            max_complexity=self.query_config.max_complexity
        )
        
        # DataLoaders (managed per request)
        self._loaders: weakref.WeakValueDictionary = weakref.WeakValueDictionary()
        
        # Circuit breakers
        self._circuit_breakers: Dict[str, CircuitBreaker] = {}
        
        # Statistics
        self._stats = {
            "queries_optimized": 0,
            "n1_problems_prevented": 0,
            "cache_savings_ms": 0,
            "total_queries": 0
        }
    
    def create_loader(self, name: str, batch_fn: Callable) -> DataLoader:
        """Create a new DataLoader."""
        loader = DataLoader(name, batch_fn)
        self._loaders[name] = loader
        return loader
    
    def get_loader(self, name: str) -> Optional[DataLoader]:
        """Get existing DataLoader."""
        return self._loaders.get(name)
    
    def execute_query(self, 
                     query: str,
                     variables: Dict = None,
                     execute_fn: Callable = None) -> Dict:
        """
        Execute query with all optimizations.
        """
        # Analyze query
        allowed, reason = self.complexity_analyzer.should_allow(query, variables)
        if not allowed:
            return {"error": reason, "allowed": False}
        
        # Check cache
        cache_key = self._generate_cache_key(query, variables)
        cached = self.cache.get(cache_key)
        if cached and self.query_config.enable_query_caching:
            self._stats["cache_savings_ms"] += 100  # Estimated savings
            return {"data": cached, "cached": True}
        
        # Execute (if function provided)
        if execute_fn:
            result = execute_fn(query, variables)
            
            # Cache result
            if self.query_config.enable_response_caching:
                self.cache.set(cache_key, result, ttl=self.query_config.cache_ttl)
            
            # Record metrics
            self._record_metrics(query, result)
            
            return {"data": result, "cached": False}
        
        return {"allowed": True, "message": "Query would execute"}
    
    def _generate_cache_key(self, query: str, variables: Dict = None) -> str:
        """Generate cache key for query."""
        content = query + json.dumps(variables or {}, sort_keys=True)
        return f"graphql:query:{hashlib.md5(content.encode()).hexdigest()}"
    
    def _record_metrics(self, query: str, result: Any):
        """Record query metrics."""
        metrics = QueryMetrics(
            query_hash=self._generate_cache_key(query),
            operation_name="anonymous",
            execution_time_ms=0,  # Would be measured in real impl
            resolver_count=0,
            db_queries=0,
            cache_hits=1,
            cache_misses=0,
            memory_bytes=0,
            error_count=0,
            timestamp=datetime.now(),
            variables={},
            complexity=QueryComplexity.LOW,
            cache_key=""
        )
        self.monitor.record_query(metrics)
        self._stats["queries_optimized"] += 1
    
    def get_dashboard_data(self) -> Dict:
        """Get all metrics for dashboard."""
        return {
            "cache": self.cache.get_stats(),
            "monitor": self.monitor.get_metrics(),
            "slow_queries": [self._format_query(q) for q in self.monitor.get_slow_queries(5)],
            "distribution": self.monitor.get_query_distribution(),
            "stats": self._stats,
            "loaders": [l.get_stats() for l in self._loaders.values()]
        }
    
    def _format_query(self, query: QueryMetrics) -> Dict:
        """Format query for display."""
        return {
            "operation": query.operation_name,
            "time_ms": query.execution_time_ms,
            "complexity": query.complexity.value,
            "cache_hit_rate": f"{query.cache_hit_rate:.1f}%"
        }
    
    def add_circuit_breaker(self, name: str, **kwargs) -> CircuitBreaker:
        """Add circuit breaker for a service."""
        cb = CircuitBreaker(name, **kwargs)
        self._circuit_breakers[name] = cb
        return cb
    
    def get_circuit_status(self) -> Dict:
        """Get status of all circuit breakers."""
        return {
            name: cb.state.value 
            for name, cb in self._circuit_breakers.items()
        }


# ============================================
# MAIN / CLI
# ============================================

def main():
    """CLI interface for testing."""
    print("=" * 70)
    print("üéØ GraphQL Performance Optimizer")
    print("=" * 70)
    
    # Create optimizer
    optimizer = GraphQLEPerformanceOptimizer()
    
    # Test DataLoader
    print("\nüì¶ Testing DataLoader...")
    
    async def batch_users(user_ids):
        # Simulate database batch fetch
        await asyncio.sleep(0.01)  # Simulate DB latency
        return [f"User-{uid}" for uid in user_ids]
    
    loader = optimizer.create_loader("users", batch_users)
    
    import asyncio
    async def test_loader():
        # Test batch loading
        results = await asyncio.gather(
            loader.load("1"),
            loader.load("2"),
            loader.load("3"),
            loader.load("4"),
            loader.load("5"),
        )
        print(f"   Results: {results}")
        print(f"   Stats: {loader.get_stats()}")
    
    asyncio.run(test_loader())
    
    # Test cache
    print("\nüóÑÔ∏è Testing Cache...")
    optimizer.cache.set("test-key", {"message": "Hello!"}, ttl=60)
    result = optimizer.cache.get("test-key")
    print(f"   Cached value: {result}")
    print(f"   Cache stats: {optimizer.cache.get_stats()}")
    
    # Test complexity analyzer
    print("\nüìä Testing Query Analyzer...")
    query = """
    {
      users {
        posts {
          comments {
            author {
              profile
            }
          }
        }
      }
    }
    """
    analysis = optimizer.complexity_analyzer.analyze(query)
    print(f"   Depth: {analysis['depth']}")
    print(f"   Complexity: {analysis['complexity']}")
    
    # Dashboard
    print("\nüìà Dashboard Data:")
    dashboard = optimizer.get_dashboard_data()
    print(f"   Total queries optimized: {dashboard['stats']['queries_optimized']}")
    
    print("\n" + "=" * 70)
    print("‚úÖ GraphQL Performance Optimizer Ready!")
    print("=" * 70)


if __name__ == "__main__":
    main()
